{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from x_transformers import XTransformer, TransformerWrapper, Encoder, Decoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from playsound import playsound\n",
    "from vector_quantize_pytorch import VectorQuantize\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.2255e-08,  1.6568e-07, -2.0146e-06, -2.5406e-07,  1.0626e-07,\n",
      "         -5.1383e-08,  2.1609e-08, -5.7241e-09,  1.0875e-10,  0.0000e+00,\n",
      "          0.0000e+00,  3.1706e-09, -2.3491e-08,  7.3499e-08, -1.8817e-07,\n",
      "          4.5257e-07, -1.7268e-06, -6.2831e-07, -8.7892e-07, -1.3882e-06,\n",
      "         -1.5320e-07, -1.7778e-06, -8.1579e-07, -1.7652e-06,  1.0746e-06,\n",
      "         -1.7143e-06,  2.3247e-06,  3.9560e-06, -7.7178e-06, -2.3971e-06,\n",
      "         -1.2847e-06, -8.0998e-08,  4.0612e-07, -1.3357e-06, -1.0618e-06,\n",
      "         -5.0726e-07, -1.8547e-06,  2.1916e-06, -2.0063e-06, -1.6240e-06,\n",
      "         -5.2815e-07,  5.0857e-07, -1.1383e-06, -1.2304e-06, -3.3296e-06,\n",
      "          1.4462e-06,  7.0002e-06, -1.8543e-06, -6.3503e-07,  2.2166e-06,\n",
      "          1.3640e-06,  3.6757e-07, -3.4094e-06, -3.2288e-06,  4.2208e-06,\n",
      "         -2.2841e-06, -4.4643e-07, -1.3056e-06,  8.2650e-07, -5.6773e-06,\n",
      "         -2.1909e-06,  5.2651e-07, -2.4907e-06,  3.9803e-06, -1.1796e-06,\n",
      "         -2.1508e-06, -1.9369e-06,  1.6417e-06, -1.5905e-07,  2.4533e-07,\n",
      "          1.3753e-06, -1.3168e-06,  7.4866e-06, -1.3192e-05,  2.9167e-05,\n",
      "         -1.5967e-04, -4.7415e-04, -4.4789e-04, -5.0460e-04, -4.0608e-04,\n",
      "         -4.2360e-04, -3.1024e-04, -2.4904e-04,  1.0564e-04,  2.0814e-04,\n",
      "          1.5077e-04,  3.8934e-05, -3.5141e-05, -1.3312e-04, -1.2567e-04,\n",
      "         -7.7857e-06, -1.8757e-04, -1.5768e-04, -1.8294e-04, -3.3028e-04,\n",
      "         -3.1637e-04, -1.5246e-04,  1.0910e-04,  2.7204e-04,  4.4238e-04,\n",
      "          4.5924e-04,  5.9159e-04,  6.2532e-04,  3.9325e-04,  4.8477e-04,\n",
      "          4.2950e-04,  2.4025e-04,  2.3461e-04,  2.0782e-04,  3.9293e-04,\n",
      "          4.2435e-04,  2.0394e-04, -1.2212e-04, -4.4066e-05, -2.3621e-05,\n",
      "         -2.0082e-04, -2.1842e-04,  5.1226e-05,  1.7844e-04,  5.8728e-06,\n",
      "         -7.6620e-05, -1.6513e-04, -2.7131e-04, -4.7615e-04, -3.3298e-04,\n",
      "         -6.9445e-05,  4.0985e-05,  1.0601e-04,  2.3498e-04,  3.9183e-04,\n",
      "         -3.2068e-05, -2.9456e-04, -3.2451e-04, -3.6690e-04, -5.7014e-04,\n",
      "         -5.6865e-04, -4.5297e-04, -1.9368e-04, -3.5490e-04, -3.7859e-04,\n",
      "         -2.6262e-04, -4.9102e-04, -5.2887e-04, -3.4583e-04,  7.9643e-05,\n",
      "          2.9766e-04,  3.5346e-04,  4.5466e-04,  4.0057e-04,  3.8743e-04,\n",
      "          2.4690e-04,  2.6348e-04,  1.7271e-04, -5.1991e-05,  4.2346e-05,\n",
      "          3.1074e-04,  2.5001e-04, -7.5387e-05, -1.2176e-04, -1.1629e-04,\n",
      "         -1.5016e-04, -5.5902e-05,  1.3757e-04,  4.4824e-04,  4.5630e-04,\n",
      "          6.0605e-04,  5.9175e-04,  3.3432e-04,  1.1324e-06, -5.1009e-05,\n",
      "         -4.5155e-05, -3.0364e-04, -3.6473e-04, -1.6316e-04,  4.5846e-05,\n",
      "         -1.4214e-04, -4.1088e-04, -1.8893e-04,  2.9101e-05, -3.1375e-04,\n",
      "         -1.2238e-04,  1.2338e-04, -2.8350e-04, -2.3793e-04, -2.2288e-05,\n",
      "          8.8945e-05, -8.3041e-05, -2.6181e-04, -9.5159e-05,  2.0210e-04,\n",
      "          6.4074e-06, -2.1430e-04,  1.7324e-05, -4.0250e-05, -4.8975e-04,\n",
      "         -6.0677e-04, -2.4043e-04, -1.4243e-04, -2.9447e-04, -8.9647e-05,\n",
      "          1.9234e-04,  4.4103e-04,  3.1181e-04,  1.5464e-04,  2.4870e-04,\n",
      "         -5.9388e-05, -1.0567e-04,  2.5694e-04,  4.0156e-04,  9.5339e-05,\n",
      "          2.0703e-04,  2.4519e-04,  2.9322e-05, -8.4015e-05, -3.4294e-04,\n",
      "          2.1389e-05,  2.7215e-04,  6.0608e-05,  5.3090e-05, -3.3368e-07,\n",
      "         -9.2017e-05, -3.9253e-05,  2.1574e-05, -4.6655e-05,  1.4411e-04,\n",
      "          2.3710e-04,  6.0316e-05,  2.2525e-04,  2.6083e-04,  1.6894e-04,\n",
      "          4.1806e-04,  5.7266e-04,  3.5674e-04,  1.3048e-05,  1.5831e-05,\n",
      "         -1.3081e-04, -3.7258e-04, -2.3814e-04, -3.7982e-04, -4.6758e-04,\n",
      "         -3.2508e-04, -5.9746e-05, -1.8677e-04, -3.6915e-04, -2.7167e-04,\n",
      "         -1.8304e-04, -3.7726e-04, -4.2927e-04, -7.6703e-05,  1.6321e-04,\n",
      "          1.6797e-05, -8.5950e-05,  1.7039e-04,  1.7136e-04, -1.0093e-04,\n",
      "         -2.2441e-04, -1.5417e-04, -2.1556e-05, -4.2842e-05, -1.3301e-05,\n",
      "         -1.6367e-04, -3.5162e-04, -5.9402e-04, -3.7052e-04, -2.9016e-04,\n",
      "         -3.3524e-04,  7.4389e-05,  3.7559e-04,  6.0078e-04,  3.6890e-04,\n",
      "          4.1629e-04,  5.1613e-04,  3.8337e-04,  4.1521e-04,  1.6824e-04,\n",
      "          8.7555e-05,  1.7832e-05,  4.6275e-05,  2.1607e-05, -6.9853e-05,\n",
      "         -1.7623e-04, -2.2398e-04,  4.8845e-05,  1.6888e-04,  1.1332e-04,\n",
      "          1.5905e-04,  2.0158e-04,  5.7864e-04,  3.9407e-04,  2.7378e-04,\n",
      "          4.6801e-04,  2.7909e-04,  2.3034e-04,  3.6300e-04,  3.5665e-04,\n",
      "          1.7389e-04,  7.2142e-05, -6.2929e-05, -2.3386e-04, -5.9870e-04,\n",
      "         -5.0880e-04, -4.2450e-04, -9.6724e-04, -1.1426e-03, -6.9577e-04,\n",
      "         -4.4387e-04, -7.9429e-05,  1.4033e-04,  2.6505e-04,  2.4905e-04,\n",
      "          2.8989e-05, -3.5817e-04, -6.5498e-04, -7.8377e-04, -1.0906e-03,\n",
      "         -9.3124e-04, -9.8914e-04, -6.6854e-04, -4.9535e-04, -8.4064e-04,\n",
      "         -6.5547e-04, -4.6798e-04, -1.6293e-04,  4.0695e-04,  9.2009e-04,\n",
      "          6.8804e-04,  9.5551e-04,  1.1623e-03,  1.2264e-03,  7.7625e-04,\n",
      "          5.1110e-04,  7.1575e-04,  5.4399e-04,  4.2777e-04,  7.5090e-04,\n",
      "          7.7247e-04,  1.5693e-04, -4.3904e-04, -4.3707e-04, -2.7028e-04,\n",
      "          7.8036e-05,  3.9503e-04,  5.4091e-04,  7.1836e-04,  6.3629e-04,\n",
      "          5.3923e-04, -3.2778e-04, -5.7882e-04, -7.0836e-04, -6.5391e-04,\n",
      "         -5.1544e-04, -3.9321e-04, -1.1165e-04, -4.5855e-04, -1.1442e-03,\n",
      "         -1.0119e-03, -1.0382e-03, -4.3609e-04,  5.1541e-04,  9.7318e-04,\n",
      "          1.1205e-03,  1.6045e-03,  1.5470e-03,  9.4070e-04,  3.3094e-04,\n",
      "         -8.1829e-05,  2.0363e-04,  7.5526e-05,  2.0024e-04,  6.6045e-04,\n",
      "          6.7710e-04, -3.1316e-04, -8.1747e-04, -1.1323e-03, -1.1917e-03,\n",
      "         -7.1548e-04, -4.1136e-05, -6.1388e-05,  2.5699e-04,  4.6306e-04,\n",
      "          2.1633e-04, -7.2383e-04, -1.5995e-03, -1.8295e-03, -1.1340e-03,\n",
      "         -6.9542e-04,  1.2604e-04,  1.3861e-04, -1.9904e-04, -3.7035e-04,\n",
      "         -6.4660e-04, -6.7463e-04, -1.0964e-04,  8.0025e-04,  1.1608e-03,\n",
      "          1.4930e-03,  2.0044e-03,  1.6387e-03,  4.5196e-04, -2.7016e-04,\n",
      "         -7.6383e-04, -8.1809e-04, -2.9838e-04,  3.3256e-04,  7.1972e-04,\n",
      "          5.0693e-05, -6.5649e-04, -8.0673e-04, -8.1024e-04, -6.9043e-04,\n",
      "          1.2545e-04,  3.7463e-04,  5.0881e-04,  1.1998e-03,  8.2208e-04,\n",
      "          1.4517e-05, -7.7777e-04, -1.6446e-03, -1.5190e-03, -9.4714e-04,\n",
      "          5.1727e-05,  9.1667e-04,  4.5301e-04, -2.4561e-04, -8.3901e-04,\n",
      "         -1.3930e-03, -9.2931e-04,  6.8195e-05,  5.2855e-04,  1.2846e-03,\n",
      "          1.9938e-03,  1.8904e-03,  9.6778e-04, -2.8121e-04, -1.2289e-03,\n",
      "         -1.7325e-03, -1.5044e-03, -6.6988e-04,  3.2490e-04,  2.1255e-04,\n",
      "         -5.2428e-04, -7.3526e-04, -1.2450e-03, -1.4532e-03, -1.8337e-04,\n",
      "          1.0733e-03,  1.5281e-03,  2.2558e-03,  2.7148e-03,  2.4178e-03,\n",
      "          1.0846e-03,  3.9733e-05, -4.9605e-04, -2.8893e-04, -1.5870e-04,\n",
      "          8.2691e-04,  1.0617e-03,  2.4713e-04, -1.5805e-04, -7.9737e-04,\n",
      "         -1.0452e-03, -2.3221e-04,  8.2644e-04,  1.3069e-03,  1.7905e-03,\n",
      "          1.8685e-03,  8.9399e-04, -4.6360e-04, -1.6500e-03, -2.6292e-03,\n",
      "         -2.2853e-03, -1.9013e-03, -1.1831e-03, -7.7815e-04, -1.1940e-03,\n",
      "         -1.2011e-03, -1.7612e-03, -2.2952e-03, -1.5052e-03,  5.1790e-05,\n",
      "          1.1294e-03,  1.4451e-03,  1.9663e-03,  1.8752e-03,  9.4300e-04,\n",
      "          5.7759e-05, -8.8814e-04, -8.2671e-04, -7.4263e-04,  2.7104e-04,\n",
      "          1.0571e-03,  5.1352e-04,  1.1436e-04, -2.3967e-04, -1.2326e-03,\n",
      "         -9.1135e-04,  6.6473e-04,  1.6605e-03,  2.2288e-03,  2.7731e-03,\n",
      "          2.6067e-03,  1.6029e-03,  1.4149e-05, -1.4148e-03, -1.5608e-03,\n",
      "         -1.3128e-03, -7.3720e-04, -1.1401e-04, -3.0979e-04, -6.1353e-04,\n",
      "         -1.0469e-03, -1.8937e-03]]) torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "filename = 'D:\\\\musicnet\\\\musicnet\\\\train_data\\\\1727.wav'\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(filename, normalize=True)\n",
    "resample_rate = 3000\n",
    "\n",
    "transformed = torchaudio.transforms\n",
    "transformed = transformed.Resample(sample_rate, resample_rate)\n",
    "resampled_waveform = transformed(waveform)[:,:512]\n",
    "print(resampled_waveform, resampled_waveform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XTransformer(\n",
    "    dim = 512,\n",
    "    enc_num_tokens = 256,\n",
    "    enc_depth = 6,\n",
    "    enc_heads = 8,\n",
    "    enc_max_seq_len = 1024,\n",
    "    dec_num_tokens = 256,\n",
    "    dec_depth = 6,\n",
    "    dec_heads = 8,\n",
    "    dec_max_seq_len = 1024,\n",
    "    tie_token_emb = True      \n",
    ")\n",
    "\n",
    "src = torch.randint(0, 1, (1, 1024))\n",
    "tgt = torch.randint(0, 1, (1, 1024))\n",
    "\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    yhat = model(src, tgt) # (1, 1024, 512)\n",
    "    yhat2 = model(src, tgt)\n",
    "    loss = criterion(yhat, yhat2)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.min(\n",
      "values=tensor([-0.0026]),\n",
      "indices=tensor([469])) torch.return_types.max(\n",
      "values=tensor([0.0028]),\n",
      "indices=tensor([499]))\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   1,   2,  15,  47,  44,  50,  40,  42,  31,  24,  10,\n",
      "          20,  15,   3,   3,  13,  12,   0,  18,  15,  18,  33,  31,  15,  10,\n",
      "          27,  44,  45,  59,  62,  39,  48,  42,  24,  23,  20,  39,  42,  20,\n",
      "          12,   4,   2,  20,  21,   5,  17,   0,   7,  16,  27,  47,  33,   6,\n",
      "           4,  10,  23,  39,   3,  29,  32,  36,  57,  56,  45,  19,  35,  37,\n",
      "          26,  49,  52,  34,   7,  29,  35,  45,  40,  38,  24,  26,  17,   5,\n",
      "           4,  31,  25,   7,  12,  11,  15,   5,  13,  44,  45,  60,  59,  33,\n",
      "           0,   5,   4,  30,  36,  16,   4,  14,  41,  18,   2,  31,  12,  12,\n",
      "          28,  23,   2,   8,   8,  26,   9,  20,   0,  21,   1,   4,  48,  60,\n",
      "          24,  14,  29,   8,  19,  44,  31,  15,  24,   5,  10,  25,  40,   9,\n",
      "          20,  24,   2,   8,  34,   2,  27,   6,   5,   0,   9,   3,   2,   4,\n",
      "          14,  23,   6,  22,  26,  16,  41,  57,  35,   1,   1,  13,  37,  23,\n",
      "          37,  46,  32,   5,  18,  36,  27,  18,  37,  42,   7,  16,   1,   8,\n",
      "          17,  17,  10,  22,  15,   2,   4,   1,  16,  35,  59,  37,  29,  33,\n",
      "           7,  37,  60,  36,  41,  51,  38,  41,  16,   8,   1,   4,   2,   6,\n",
      "          17,  22,   4,  16,  11,  15,  20,  57,  39,  27,  46,  27,  23,  36,\n",
      "          35,  17,   7,   6,  23,  59,  50,  42,  96, 114,  69,  44,   7,  14,\n",
      "          26,  24,   2,  35,  65,  78, 109,  93,  98,  66,  49,  84,  65,  46,\n",
      "          16,  40,  92,  68,  95, 116, 122,  77,  51,  71,  54,  42,  75,  77,\n",
      "          15,  43,  43,  27,   7,  39,  54,  71,  63,  53,  32,  57,  70,  65,\n",
      "          51,  39,  11,  45, 114, 101, 103,  43,  51,  97, 112, 160, 154,  94,\n",
      "          33,   8,  20,   7,  20,  66,  67,  31,  81, 113, 119,  71,   4,   6,\n",
      "          25,  46,  21,  72, 159, 182, 113,  69,  12,  13,  19,  37,  64,  67,\n",
      "          10,  80, 116, 149, 200, 163,  45,  27,  76,  81,  29,  33,  71,   5,\n",
      "          65,  80,  81,  69,  12,  37,  50, 119,  82,   1,  77, 164, 151,  94,\n",
      "           5,  91,  45,  24,  83, 139,  92,   6,  52, 128, 199, 189,  96,  28,\n",
      "         122, 173, 150,  66,  32,  21,  52,  73, 124, 145,  18, 107, 152, 225,\n",
      "         271, 241, 108,   3,  49,  28,  15,  82, 106,  24,  15,  79, 104,  23,\n",
      "          82, 130, 179, 186,  89,  46, 164, 262, 228, 190, 118,  77, 119, 120,\n",
      "         176, 229, 150,   5, 112, 144, 196, 187,  94,   5,  88,  82,  74,  27,\n",
      "         105,  51,  11,  23, 123,  91,  66, 166, 222, 277, 260, 160,   1, 141,\n",
      "         156, 131,  73,  11,  30,  61, 104, 189]], dtype=torch.int32)\n",
      "torch.Size([1, 512, 2000])\n"
     ]
    }
   ],
   "source": [
    "encoder = TransformerWrapper(\n",
    "    num_tokens = 2000,\n",
    "    max_seq_len = 512,\n",
    "    attn_layers = Encoder(\n",
    "        dim = 512,\n",
    "        depth = 12,\n",
    "        heads = 8\n",
    "    )\n",
    ")\n",
    "\n",
    "x = torch.randint(0, 256, (1, 1024))\n",
    "# mask = torch.randint(0, 1, x.shape).bool()\n",
    "print(torch.min(resampled_waveform, axis=1), torch.max(resampled_waveform, axis=1)) \n",
    "wv = np.abs(resampled_waveform * 100000).int()\n",
    "print(wv)\n",
    "# print(initial_melody, x)\n",
    "encoding = encoder(wv)\n",
    "print(encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6011, 0.4332, 0.4064,  ..., 0.5424, 0.9954, 0.1035]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-d01105e81c2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\user\\desktop\\python_x64\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\desktop\\python_x64\\lib\\site-packages\\x_transformers\\x_transformers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, return_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_emb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_emb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\desktop\\python_x64\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\desktop\\python_x64\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32mc:\\users\\user\\desktop\\python_x64\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2042\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2044\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "decoder = TransformerWrapper(\n",
    "    num_tokens = 2000,\n",
    "    max_seq_len = 512,\n",
    "    attn_layers = Decoder(\n",
    "        dim = 512,\n",
    "        depth = 12,\n",
    "        heads = 8\n",
    "    )\n",
    ")\n",
    "\n",
    "x = torch.rand((1, 1024))\n",
    "\n",
    "decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq = VectorQuantize(\n",
    "    dim = 512,\n",
    "    codebook_size = 256,\n",
    "    codebook_dim = 32,\n",
    "    use_cosine_sim = True#  paper proposes setting this to 32 or as low as 8 to increase codebook usage\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 2048, 512)\n",
    "quantized, indices, commit_loss = vq(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
